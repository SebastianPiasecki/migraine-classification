{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelANN(object):\n",
    "    \"\"\"\n",
    "    Simple Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, inNum=2, hNum=2, lr=0.01, job_type=\"class\"):\n",
    "        \"\"\"\n",
    "        :param iNum: number of neurons in input layer\n",
    "        :param hNum: number of neurons in hidden layer\n",
    "        :param lr: leraning rate\n",
    "        \"\"\"\n",
    "        # inicjalizacja learnig rate'u\n",
    "        # wielkość ta parametryzuje wielkość korekt wag sieci\n",
    "        self.lr = lr\n",
    "        \n",
    "        # ilość wejść\n",
    "        self.inNum = inNum\n",
    "        # ilość wyjść\n",
    "        self.outNum = 1\n",
    "        \n",
    "        #inicjalizacja wag\n",
    "        #warstwa wejściowa to same wejścia, wszystko leci na ukrytą\n",
    "        \n",
    "        # warstwa ukryta, hNum neuronów, po dwa wejścia na każdy neuron\n",
    "        self.W_h = tf.Variable(tf.random.normal([self.inNum,hNum]))\n",
    "        self.b_h = tf.Variable(tf.zeros([hNum]))\n",
    "        \n",
    "        # warstwa wyjściowa, self.outNum neuron z hNum wejściami\n",
    "        self.W_out = tf.Variable(tf.random.normal([hNum,self.outNum]))\n",
    "        self.b_out = tf.Variable(tf.zeros([self.outNum]))\n",
    "\n",
    "        self.job_type = job_type\n",
    "        self.outSize = 1\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input data, 2D - [batch, features], batch could be None, also could be one row of feature, but also 2D\n",
    "        \"\"\"\n",
    "        # feed forward\n",
    "        # wejściowa warstwa\n",
    "        layer_in = x\n",
    "        # warstwa ukryta\n",
    "            # mnożymy przez wagi i dodajemy biasy\n",
    "        layer_hidden = tf.add(tf.matmul(layer_in, self.W_h), self.b_h)\n",
    "            # przechodzimy z sygnałem przez funkcję aktywacji\n",
    "        layer_hidden = tf.nn.relu(layer_hidden)\n",
    "        \n",
    "        # warstwa wyjściowa\n",
    "            # mnożymy przez wagi i dodajemy biasy\n",
    "        layer_out = tf.add(tf.matmul(layer_hidden, self.W_out), self.b_out)\n",
    "            # przechodzimy z sygnałem przez funkcję aktywacji\n",
    "            # sigmoid z tego względu, aby otrzymać znormalizowane wyjście w zakresie 0-1\n",
    "        layer_out = tf.nn.sigmoid(layer_out)\n",
    "        return layer_out\n",
    "\n",
    "    def weightsUpdate(self, dW_h, db_h, dW_out, db_out):\n",
    "        \"\"\"\n",
    "        :param dW_h:  hidden layer weights derivative\n",
    "        :param db_h:  hidden layer bias derivative\n",
    "        :param dW_out:  out layer weights derivative\n",
    "        :param db_out:  out layer bias derivative\n",
    "        \"\"\"\n",
    "        # korekta wag warstwy ukrytej\n",
    "        self.W_h.assign_sub(self.lr * dW_h)\n",
    "        # korekta biasu warstwy ukrytej\n",
    "        self.b_h.assign_sub(self.lr * db_h)\n",
    "        # korekta wag warstwy wyjściowej\n",
    "        self.W_out.assign_sub(self.lr * dW_out)\n",
    "        # korekta biasu warstwy wyjściowej\n",
    "        self.b_out.assign_sub(self.lr * db_out)\n",
    "    \n",
    "    def lossFunc(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        :param y_pred: predicted data, from model\n",
    "        :param y_true: true output from model\n",
    "        \"\"\"\n",
    "        # zmiana kształtu referencji na potrzeby oblcizeń macierzowych\n",
    "        y_true = tf.reshape(y_true, (-1, self.outSize))\n",
    "        if self.job_type == \"class\":\n",
    "            return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_true, y_pred))\n",
    "        elif self.job_type == \"regr\":\n",
    "            return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "    \n",
    "    def fit(self, x, y_true):\n",
    "        \"\"\"\n",
    "        :param x: training data\n",
    "        :param y_true: connected to x, real output\n",
    "        \"\"\"\n",
    "        # definicja taśmy\n",
    "        with tf.GradientTape() as t:\n",
    "            # wyznaczenie błędu\n",
    "            # tutaj jest uruchomiony cały model i policzony błąd\n",
    "            # więc pod taśmę wchodzi cały model wraz z funkcją błędu !!!!\n",
    "            y_pred = self.predict(x)\n",
    "            current_loss = self.lossFunc(y_pred, y_true)\n",
    "        # odwijamy taśmę i otrzymujemy pochodne\n",
    "        dW_h, db_h, dW_out, db_out = t.gradient(current_loss, [self.W_h, self.b_h, self.W_out, self.b_out])\n",
    "        # korygujemy wagi\n",
    "        self.weightsUpdate(dW_h, db_h, dW_out, db_out)\n",
    "        # błąd zwracamy do świata zewnętrznego\n",
    "        return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age            0\n",
       "Duration       0\n",
       "Frequency      0\n",
       "Location       0\n",
       "Character      0\n",
       "Intensity      0\n",
       "Nausea         0\n",
       "Vomit          0\n",
       "Phonophobia    0\n",
       "Photophobia    0\n",
       "Visual         0\n",
       "Sensory        0\n",
       "Dysphasia      0\n",
       "Dysarthria     0\n",
       "Vertigo        0\n",
       "Tinnitus       0\n",
       "Hypoacusis     0\n",
       "Diplopia       0\n",
       "Defect         0\n",
       "Ataxia         0\n",
       "Conscience     0\n",
       "Paresthesia    0\n",
       "DPF            0\n",
       "Type           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Typical aura with migraine' 'Migraine without aura' 'Basilar-type aura'\n",
      " 'Sporadic hemiplegic migraine' 'Familial hemiplegic migraine' 'Other'\n",
      " 'Typical aura without migraine']\n"
     ]
    }
   ],
   "source": [
    "cat_values = df[\"Type\"].unique()\n",
    "print(cat_values)\n",
    "is_migraine = [\n",
    "    'Typical aura with migraine', \n",
    "    'Migraine without aura', \n",
    "    'Basilar-type aura', \n",
    "    'Sporadic hemiplegic migraine',\n",
    "    'Familial hemiplegic migraine'\n",
    "]\n",
    "no_migraine = ['Other', 'Typical aura without migraine']\n",
    "df_binary = df\n",
    "\n",
    "df_binary[\"Type\"] = (df_binary[\"Type\"].isin(is_migraine))\n",
    "# print(df_binary.head())\n",
    "# print(df_binary.dtypes)\n",
    "# df_binary[\"Type\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_binary[\"Type\"]\n",
    "X = df_binary[[\"Age\", \"Duration\", \"Frequency\", \"Location\", \"Character\", \"Intensity\", \"Nausea\", \"Vomit\", \"Phonophobia\", \"Photophobia\", \"Visual\", \"Sensory\", \"Dysphasia\", \"Dysarthria\", \"Vertigo\", \"Tinnitus\", \"Hypoacusis\", \"Diplopia\", \"Defect\", \"Ataxia\", \"Conscience\", \"Paresthesia\", \"DPF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Count: 320\n",
      "Testing Data Count: 80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "# y_train = np.asarray(y_train).astype('float32')\n",
    "# y_test = np.asarray(y_test).astype('float32')\n",
    "print('Training Data Count: {}'.format(X_train.shape[0]))\n",
    "print('Testing Data Count: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train, y_train)\n",
    "# X_train = np.asarray(X_train).astype('float32')\n",
    "X_test = sc.fit_transform(X_test, y_test)\n",
    "# X_test = np.asarray(X_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05, 6, 0.9125)\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "epochs = 20\n",
    "X_train, X_test = X_train.astype('float32'), X_test.astype('float32')\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hNum in range(2, 11):\n",
    "        # print(\"Number of hidden neurons in first layer per input neuron: \", hNum)\n",
    "        # print(\"Learning rate: \", lr)\n",
    "        ann = modelANN(inNum=X_train.shape[1], hNum=hNum, lr=lr)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            curr_loss = ann.fit(X_train, y_train)\n",
    "\n",
    "        # sprawdzam accuracy modelu na danych testowych (20% z datasetu)\n",
    "        y_pred = ann.predict(X_test)\n",
    "        y_pred=(y_pred >= 0.5) # inne threshold warto sprawdzic\n",
    "        temp_result = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "        results.append((lr, hNum, temp_result))\n",
    "        # print(\"Accuracy: \", temp_result)  \n",
    "\n",
    "best_result = results[0]\n",
    "for r in results:\n",
    "    if r[2] > best_result[2]:\n",
    "        best_result = r\n",
    "print(best_result)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84e7ca4d3cda02558b8488111a750648cb6a4b3668a23f44e278fc8092a466ab"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
